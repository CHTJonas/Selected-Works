\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[hidelinks=true]{hyperref}
\title{To What Extent is Black Hole Complementarity the Most Viable Solution to the Black Hole Information Paradox?}
\author{Charlie Jonas\\The Bishop's Stortford High School\\\\Candidate Number: 8112\\Centre Number: 17201}
\date{November 15, 2015\\Third Revision}
\begin{document}

\maketitle

\begin{abstract}
I give an overview of the scientific method and the background physics of the black hole information loss paradox, as well as a brief historical overview of its formulation. I examine three of the main postulated solutions; black hole complementarity, baby universes and Planckian remnants; and discuss their relative merits and downfalls when compared to each other. In their present form, I find that all the proposed solutions I review are in some way inadequate but conclude that the theory of information residing in baby universes is the most viable solution, with no prejudice with respect to a correct theory ever being proved.
\end{abstract}

\pagebreak
\tableofcontents
\pagebreak

\section{Introduction}
The black hole information paradox is a contradiction that arises when two of the fundamental theories of physics, quantum mechanics and gravity (specifically, relativity), are both used to make a prediction about the nature of black holes. The paradox was first discovered when Jacob Bekenstein and Stephen Hawking published a paper in 1974 which appeared to show that black holes â€˜evaporateâ€™ away by emitting particles that would cause them to lose mass (in this sense and throughout my extended project unless otherwise noted, evaporate simply means â€˜to radiate awayâ€™ and is completely unconnected to the process of a liquid boiling off into a gas). This violated a firmly entrenched idea held by physicists that the information of a physical system can never be erased or lost. Despite the proposal of several postulates and conjectures, there is still no universally accepted solution to the problem. Indeed, there is still an ongoing bet between Kip Thorne and John Preskill on what the solution to the problem is, with Hawking himself having conceded the bet earlier. Current research into this field is limited by our inability to directly observe black holes; however, if the proposed theories are correct then the Large Hadron Collider at the European Organization for Nuclear Research (CERN) may be able to create and observe micro-black holes which might provide some insight \cite{Choptuik:2009ww}. Hawking also tells us that if we were to build an atomic bomb with all the deuterium on Earth then the resulting explosion would compress matter to such a degree that a black hole would be created \cite{hawking1998brief} however the explosion would probably destroy the earth too, rendering the experiment slightly infeasible. Additionally, no calculation data is given and no references to John Wheeler, the physicist responsible for this idea, are given. We should therefore exclude this as a possible avenue of research to confirm any scientific theories.

In this report I will first describe and explain the necessary underlying physics that is needed to understand the formation of black holes and how the information paradox arises. I will then evaluate what to date is the most widely recognised postulate for a solution â€“ black hole complementarity â€“ and then compare and contrast the other main postulates put forward by theoretical physicists â€“ Information Escape, Baby Universes and Remnants â€“ to see which one can be considered the most viable at present.

\section{The Scientific Method}
To come to a conclusion it is necessary to define `viable' and discuss the scientific method in greater detail. Hawking \cite{hawking1998brief} says that ``a theory is just a model of the universe, or a restricted part of it, and a set of rules that relate quantities in the model to observations we make.'' Richard Feynman \cite{feynman1985qed} tells us that ``[Physicists] have learned to realise that whether they like a theory or they donâ€™t like a theory is \textit{not} the essential question. Rather, it is whether or not the theory gives predictions that agree with experiment.''

Both these famous physicists and authors in their own rights conclude that any scientific theory must agree with evidence. A scientist cannot ask why the universe behaves in a certain way but instead must ask how it is behaving in this way. It is this principle that forms the basis of the scientific method â€“ the methodology according to which all research is carried out: formulate a theory; make a prediction; test that prediction to see if it agrees with experimentation; refine the theory.
For the purposes of this extended project, the usual scientific method presents a problem. No one has yet directly observed a black hole and no one ever will. The vast distance scales involved in observing objects we believe to be black holes prevent accurate readings being taken and due to the very nature of black holes, it is impossible to relay some measurements of the entity. To combat this problem I will use a widely-cited principle known as Ockhamâ€™s razor which states ``when you have two competing theories that make exactly the same predictions, the simpler one is the better.'' \cite{occam'sRaz} In my case this will mean that the theory that requires the smallest addition of â€˜new physicsâ€™ and the smallest paradigm shift will be the one that is the most viable. This principle is in widespread use amongst philosophers and cosmologists and therefore I can use it to come to a conclusion about reliability. As with any theory however, it is only as good as the evidence that it is based on and with time, new phenomena may come to light which cast doubt on whether the theory I have chosen as being most viable, still is.

Most of the sources I reference are peer-reviewed scientific journals. Peer-reviewing is used to ensure that the conclusions made by the authors are valid \& reliable and that any data is verifiable by independent experiment and analysis. Some eminent journals such as the Physical Review series which are published by the American Physical Society are highly respected and carry an even greater degree of reliability than others. Citations are in the IEEE style which is the most commonly used style in scientific papers. All bibliographical information is listed at the end of the report.

\section{Background Physics}
\subsection{Quantum Mechanics}
Before quantum mechanics, the universe was largely considered by physicists to be clockwork and deterministic, meaning that there was a set of laws which governed the universe and it was possible to compute exactly how the universe would behave provided that you knew the state of the universe at one time with exact precision \cite{hawking1998brief}. This reasoning followed from widely tested and irrefutable physical theories such as Newton's theory of gravitation, Kepler'€™s laws of planetary motion, Hook'€™s law for springs and Coulomb'€™s law for electric attraction. The idea of determinism was strongly resisted by those of a religious objection who considered this an impingement on God'€™s divine will; however, disproof of determinism came indirectly from German physicist Max Planck with his theory that electromagnetic waves are emitted in quanta with energy directly proportional to their frequency. His equation for this relationship is one of the most famous in physics:
$ E=hf $.
This led to direct reasoning by Werner Heisenberg who formulated his famous uncertainty principle. Expressed succinctly this states that increasing the accuracy in the measurement of the position of a particle decreases the accuracy in the measurement of the same particleâ€™s velocity. Both of these new theories have been tested repeatedly over the last century and have yet to be disproved by any experiment. They are now accepted as fundamental concepts of modern physics by the scientific community. This signalled the end for determinism - how can a theory that contains inherent uncertainty be used to predict the future or past with absolute certainty?

In the 1920s, Paul Dirac, Erwin SchrÃ¶dinger and Heisenberg set about reformulating physics into a new theory to explain all these ideas â€“ quantum mechanics. A small particle was no longer considered as a point of matter but instead as a wave function with an associated probability density.

\subsection{Relativity}
In 1905, around the time the quantum revolution was happening, Albert Einstein proposed his theory of special relativity. This was a revolution for physics because it proposed the idea that the speed of light is constant for all reference frames. In simpler terms it meant that however fast you were moving, the speed that light travelled was always the same. The implications of this are that time of a moving object appears to slow down to a stationary observer. If Bob stays on earth and his twin Alice flies off and later returns in a rockets at a speed very close to the speed of light then Alice will be a lot younger than Bob as time has dilated in her reference frame. The amount that time dilates relative to a stationary observer is given by the equation 
\begin{align}
\gamma = \frac{1}{\sqrt{1-\frac{v^2}{c^2}}}
\end{align}
which is known as the Lorentz factor \cite{forshaw2009dynamics}.

\subsection{Black Holes}
The first people generally considered to have postulated the existence of black holes are Pierre-Simon de Laplace and John Michell \cite{susskind2008black}. They thought that, since Isaac Newton had proposed the corpuscle particle theory of light, light must be affected by gravity. They proposed that there could be stars with mass and density large enough that light could not escape their gravitational pull. Such stellar object would be completely dark and unobservable.

Physicists now know that when a large star of mass about five times that of the sun come to the end of its life cycle it will collapse under its own weight and form a singularity, a single point in space-time of infinite density. The closer an object gets to this singularity, the greater the force between it and the object. At some point the acceleration due to the force will be such that not even photons, the particles that make up light and all other forms of electromagnetic radiation, will be able to escape from the black hole and will be sucked in. Using Newtonâ€™s law of gravity and the constant acceleration formulae (both of these are well-documented and have been widely tested by physicists and mathematicians), the escape velocity for a particle moving away from a body can be computed. Since photon travels at the same speed throughout the universe the equation can be solved for that velocity to give the Schwarzschild radius
$ R_s=2MG/c^2 $
where $ M $ is the mass of the black hole, $ G $ is the gravitational constant and $ c $ is the speed of light \cite{Majumdar:1998xv}. The Schwarzschild radius is also known as the event horizon, the point at which nothing can return from a black hole.

\subsection{Information}
The conservation of energy is fundamental to all sciences and is one of the most accurately confirmed facts of nature. Similarly, there is another law governing the conservation of information which states that if you know the present with perfect precision you can predict the future for all time and be absolutely sure of the past \cite{susskind2008black}. However this disagrees with the postulates of quantum mechanics which state that the universe isnâ€™t deterministic. This apparent contradiction is resolved by the fact that the uncertainty can be reversed provided the particle isnâ€™t disturbed. Take a photon, for example. If it moves from A to B it will take a random path. If we reverse that motion (run time backwards) then it will arrive back at A provided we make no measurement of its position or velocity, the critical parts of the uncertainty principle mentioned earlier. In this way the information of a physical system cannot be destroyed or â€˜lostâ€™, provided that no explicit measurements of that system are made.

\subsection{Entropy}
Crudely speaking, entropy is a measure of disorder. That is to say ``entropy is a measure of the number of arrangements that conform to some specific recognisable criterion'' \cite{susskind2008black}. Entropy is related to the number of bits of information in a system and it follows that a disorganised random arrangement has greater entropy than an ordered one as there will be fewer states that bear resemblance to the ordered arrangement. This result, that greater disorder means greater entropy, is important throughout statistical and thermal physics.
Take a crystal lattice for example. The atoms are arranged in a regular, repeating and ordered pattern which gives the system low entropy. Upon heating through the liquid phase to a gas, the particles break from the lattice and move around with high kinetic energies, filling the container and bouncing off its walls. This state corresponds to higher entropy and gives another important result â€“ increasing temperature or increasing energy increases the entropy of a system. In this sense entropy is a measure of hidden information.

The second law of thermodynamics states that entropy in an isolated system must always increase \cite{hawking1998brief,susskind2008black,planck2013treatise}. This law is so widely quoted and cited that it is taken as fact. Much research has taken place in the field of thermal physics and statistical mechanics and no process has yet been found (other than the black hole information paradox) that violates this law. We can therefore trust its reliability to the utmost degree.


\section{What is the Black Hole Information Paradox?}
In order to come to a conclusion as to which postulated solution to the information paradox is the most viable we must first understand the history behind the information paradox.

\subsection{Black Hole Entropy}
The existence of a black hole information paradox was first discovered by Jacob Bekenstein when he used the second law of thermodynamics to prove wrong an old assumption about black holes. Originally, black holes were thought to be observably featureless, meaning that their surface (the event horizon) was completely smooth and a uniform perfect circle. Apart from the overall electric charge on a black hole, its mass and rotational speed, all black holes were thought to be alike. Bekenstein proposed a scenario in which hot, entropic gas enters the event horizon of a black hole, becoming forever hidden from the rest of the universe. If the black hole were featureless it would hide this entropy and it would be unobservable, meaning that the entropy of the universe would decrease, violating the second law of thermodynamics \cite{susskind2008black}. To solve this, Bekenstein proposed that black holes must have their own entropy, and that this must in some way be proportional to their mass and therefore their area \cite{PhysRevD.7.2333}.

Using the principles of quantum mechanics Bekenstein concluded that adding one bit of information to a black hole will increase its area by one Planck unit of area. This relationship was found to violate another fundamental relativistic law, the Einstein Equivalency Principle, which stated that for objects entering a black hole there should be no discernible change in surroundings.

\subsection{Hawking Radiation}
The next stage in the construction of the information paradox was the discovery by Stephen Hawking that black holes radiate energy and therefore, according to Einsteinâ€™s famous equation $ E=mc^2 $, lose mass \cite{EinsteinPaper}. This means that over time all black holes shrink, provided they lose more mass due to radiation than gain by consumption of matter falling over the event horizon.

All matter in the universe that completely absorbs light incident upon it is termed a â€˜black bodyâ€™ and as any light falling on a black hole cannot escape from the event horizon, a black hole falls under this category too. As black bodies do not reflect electromagnetic radiation they must absorb the energy that the photons carry. This in turn raises their temperature and makes them glow in the same way that a metal rod placed in a fire and heated glows red hot \cite{planck2013theory}. Using this thermodynamic property, Hawking reasoned that black holes must have a temperature as they do not reflect any light that falls into the event horizon and, crucially, that they must emit radiation similarly to all other black bodies \cite{susskind2008black,Hawking:1974sw}. These photons later became known as â€˜Hawking radiationâ€™ and the temperature of a black hole is given by the equation \cite{hawking2001universe}
\begin{align}
T=\frac{c^3 h}{16\pi^2 GMk}
\end{align}

This shows that the temperature is inversely proportional to the mass, so larger black holes have a lower rate of emission of Hawking radiation and so loose mass at a slower rate. The entropy of a black hole is given by \cite{hawking2001universe}
\begin{align}
S=\frac{Akc^3}{4\hbar G}
\end{align}
where $ S $ is the entropy, $ A $ is the area of the event horizon, $ k $ is the Boltzmann constant and $\hbar = h/2Ï€ $.

\subsection{Information Paradox}
This is the point at which the information paradox arises. Without Hawkingâ€™s revelation that black holes evaporate away, any particle that fell into a black hole would be forever trapped within it. This concept, although condemning the particle to eternal confinement, is not a problem to physicists as the entropy and therefore the information about that particle is effectively hidden but crucially not destroyed. It simply exists out of our grasp. However when Hawking radiation is taken into account black holes evaporate away and completely disappear along with the information contained within, leaving no remains. This leads to a violation of information conservation which states that information cannot be destroyed.

Hawking \cite{Preskill:1992tc} maintained that the usual laws of physics did not apply in this situation and used maths to demonstrate that information will be lost when a highly unusual gravitational field around a mass is encountered \cite{Hawking:2005kf}. Others were doubtful and Susskind pointed out that loosing information is the same as generating entropy and therefore heat \cite{susskind2008black}. If information were lost then black hole evaporation would cause empty space to heat up by colossal amounts, up to a ``thousand billion billion billion degrees in a tiny fraction of a second''. Whilst Susskind is a widely respected physicist who has written many papers in prolific journals, the evidence in questions comes from a popular book which is written largely devoid of any references. Whilst Susskind is unlikely to falsify this information, care needs to be taken to cross-check his evidence with other authors, especially later in this project.

Now that we have the foundations of the black hole information paradox in place I will go into detail about some of the postulated solutions. I shall start with a brief introduction to each theory and then a discussion of its validity and viability.

\section{Information Escape}
This is a very simple solution â€“ to say that there is no paradox because the information comes out in the Hawking radiation. Unfortunately this is one of the easiest solutions to disprove and is almost universally rejected by physicists.

Page tells us that the this form of information escape is impossible because information contained in the quanta that come from a black hole is in a mixed state \cite{PhysRevLett.44.301}, meaning that it contains information about the thermal entropy inside the black hole as well as its quantum states. This affects the information as it gives details about the internal state of the black hole which is irrelevant; we only want to know the information that was â€˜thrown inâ€™. Preskill \cite{Preskill:1992tc} shows that this is not a problem as over a period of time one can measure correlations between quanta emitted early on and later to recover the information. However he states that this would require the black hole to store extra information that cannot be present about what had been already emitted in quanta and what remained to be emitted in order to induce these correlations. This impossibility shows that information escape cannot be considered as a viable solution unless there is some other physical mechanism in effect to create the correlation information. However this would fall foul of Ockhamâ€™s razor which states that the addition of new physics complicates the theory and that simpler solutions should be preferred if possible. Preskillâ€™s argument is given in a pre-print to a report appearing in a conference and as such is likely to be well-researched and extensively referenced so as to avoid possible humiliation at the conference for presenting ideas which disagree with evidence or theory. Although he does come to a conclusion and eliminate certain theories he considers untenable, his paper reviews several possible solutions to the information paradox and so is likely to be reasonably neutral in that respect. As a pre-print, this article was not peer reviewed in the same way a journal article would be. Therefore it has not been subjected to the scrutiny of independent scientists and so may be unreliable. However I believe that as it was written for a conference, Pageâ€™s article would have had to be well-researched and sourced, therefore reducing the likelihood of unreliability. In this case cross referencing with other authors is a good idea.

This idea is also backed up by Susskind in his book â€˜The black hole warâ€™ where he tells the reader that information escaping is implausible \cite{susskind2008black}. However, his argument is not at all detailed and so is very unconvincing. Furthermore, the book itself holds no references or sources. His 1992 paper \cite{Susskind:1992gd} on the subject however explains the argument better by stating that there must be a mechanism to strip information from matter falling over the horizon before it reaches the singularity for the information to be encoded in Hawking radiation. He agrees with Preskill who states that there is no evidence for this mechanism as the black hole horizon has ``no hair'', so matter or observers falling over it would not notice any change in surroundings. Susskind gives his analysis of multiple solutions to the paradox so his evidence is likely to be balanced however he makes clear from the start that ``more conservative alternatives should be exhausted before we modify quantum mechanics'' which leads us to conclude that he may be slightly biased, despite his acceptable use of Ockham Razor to preclude the addition of complicated new physics if at all possible. The consensus against information escape between Preskill and Susskind shows that information escape is unlikely to be the most viable solution as it would require the addition of new physics which is less favourable under Ockhamâ€™s razor.

\section{Black Hole Complementarity}
Black Hole Complementarity is the main theory I will be examining to determine if it is the most viable. It is based on apparent differences in the observed outcome of information that falls into a black hole, dependant on whether an observer is inside or outside the black hole horizon. Complementarity has its roots in quantum mechanics where it was used by early physicists to explain wave-particle duality. Since they observed phenomena in their experiments where matter acted as particles and as waves, Niels Bohr realised that complementarity meant that matter acted as waves or particles but not both at the same time \cite{susskind2008black}. Hence, complementarity in black hole terminology is when information can have one of two different outcomes, dependant on the observer, but not both at the same time.

\subsection{Supporting Evidence}
Leonard Susskind, one of three pioneers of the theory of complementarity, tells us that we can only record data relating to the information in black holes from one side of the event horizon. He likens the effect to that of measuring some quantity outside and then again but simultaneously inside; there is no way to get a message from inside the horizon to the outside as it would have to exceed the speed of light. Therefore it is impossible for the observers to obtain both copies of the same reading, so neither would realise the other had made a different measurement \cite{Susskind:1993if}. The possible circumstance that an observer outside makes a measurement and then falls into the black hole such that there are now two of the same measurement from different sides of the horizon is prevented by the fact that the time taken to make a measurement from outside and then get that measurement inside is longer than the time taken for the measurement inside to hit the singularity \cite{Page:1993up}.

Susskind says that the paradox is avoided due to the existence of a very hot and very thin layer just above the event horizon, the stretched horizon, which is responsible for scrabbling information falling into the black hole. Since measuring particles falling into or through the stretched horizon would require using energetic photons of very short wavelengths then this would, by the uncertainty principle, cause the particles to be scattered all over the stretched horizon, thereby negating any measurement. Black hole complementarity postulates that information falling into the black hole heats this layer to high temperatures and it is this that radiates heat in the form of Hawking radiation \cite{susskind2005introduction}.

The theory of black hole complementary also relies on a property of string theory known as the holographic principle. String theory states that all particles are small loops of fundamental strings, perhaps no bigger than the Planck length. The fluctuations in quantum physicist, when applied to these strings, cause them to vibrate violently and fill the space of the entire universe. The interpretation of this is that all the information about the particle is then stored at the boundary of that string, in other words at the edge of the universe. This is the holographic principle â€“ that information in any volume of space is limited to that that can be stored on the boundary of that volume \cite{Susskind:1994vu,'tHooft:1993gx}. Susskind \cite{susskind2008black} tells us that the holographic principle ``has become a part of the mainstream theoretical physics. It is no longer just a speculation about quantum gravity.'' At face value, the holographic principle is unlikely to be viable when compared to other solutions because not only would it require new physics but also a complete paradigm shift. Applying Ockhamâ€™s razor, we find that this should be a less preferred solution if another simpler one is available.

In analysing Susskindâ€™s evidence it is important to remember that he has been sceptical of the idea of information loss since the early days when the paradox was first conjectured. The title of his book ``the black hole war'' might lay weight to a possible claim that he is biased against other feasible and possible solutions. Furthermore this book, which is a substantial reference source for this report, is written for a more general audience and so does not contain a list of references and sources. Susskind does however make footnotes on occasions where the reader wishes to read further and his full scientific papers on the subject do contain extensive referencing and extra details which can be cross checked. Gerard â€˜t Hooft is another supporter of black hole complementarity and is a source we can perhaps trust more. He is quoted \cite{hooft1997search} as rejecting string theory, saying that he ``would not be prepared to call string theory a â€˜theoryâ€™, rather a â€˜modelâ€™ or not even that: just a hunch.'' Since he disagrees with the model but still sees it as a legitimate solution to the information paradox we can trust that his and Susskindâ€™s theory is at theoretically valid solution. As work on black hole complementarity was carried out in several stages by physicists from different institutions with different sources of funding and printed in different journals, and often in teams with multiple authors on the published papers, we are able to trust the conclusions they have come to as there would have been an extensive peer review process and plenty of analysis of the theory and mathematics behind complementarity. The opportunity for bias would be slim as there are so many people working together.

\subsection{Contradictory Evidence}
Peter Bokulich \cite{pbok} argues that whilst an observer falling through the horizon should not be able to detect the stretched horizon (due to the Einstein equivalence principle), reconciling this idea is a lot more complicated than a ``naÃ¯ve'' comparison to the early days of quantum theory. He adds that black hole complementarity is weak in the sense that it fails to address the limitations of current ``effective theories''. Bokulich also states that he disagrees with their dismissal of quantum field theory, arguing that the energy of matter and black holes are typically in ranges that are better suited to a field theory which offers solutions to small-scales in a different way. As a philosopher we should be careful about entirely trusting Bokulichâ€™s judgement of physics problems. Whilst he does give note to other scientists who support his position his claims are not directly sourced and so must be questioned and considered unreliable.

Joe Polchinski \cite{guestpost} states that some of the postulates made by Susskind cannot all be true simultaneously and that there is no feasible mechanism for copying the information crossing the horizon. Here, Polchinskiâ€™s source is a webpage blog for a popular magazine. Unfortunately it contains no references and as it was not written by Polchinski himself its reliability must be questioned. Also, Polchinskiâ€™s criticism of the mechanism for copying information at the horizon goes against the scientific research principle which states that physicists should not question why something happens (in this case information copying) when the mathematics of the theory demonstrate that it occurs. This may add even more to the unreliability of the article in question. Despite all the criticism, the idea that another physicist agrees does provide some backup to Bokulichâ€™s claims against black hole complementarity.

One of the main difficulties that complementarity has to overcome is that posed by Ockhamâ€™s razor. The holographic principle tells us that information in a volume of space is actually stored on its boundary. When you extend this concept to the universe (the universe is a volume of space that is expanding at an ever-increasing rate \cite{Riess:1998cb}\cite{camlecture}) we find that all the information about matter in our universe is stored at the edge of the universe. This is a drastic new concept for modern physics and is a huge paradigm shift from what we currently know. As Ockhamâ€™s razor tells us to reject a more complicated theory over one that is less complex, black hole complementarity may not fare well against other theories. That is not to say that it is incorrect â€“ remember that the scientific method tells us that proof of a theory is based on measurement and observation â€“ however other theories may offer more viable solutions.

\section{Baby Universes}
The idea with this theory is that the information lost in the black hole evaporation is instead transferred to another universe, perhaps specific to each black hole or common to two or more. The theory relies on using quantum gravity to prevent the creation of an absolute singularity at the centre of the black hole \cite{Preskill:1992tc}.

\subsection{Supporting Evidence}
Zel'dovich \cite{Zeldovich:1976vq} tells us that ``Hawkingâ€™s theory leads to the conclusion that it is possible that a small closed universe splits off spontaneously from our universe.'' Indeed, Hawking himself has spoken about the possibility of imaginary time flowing in the space that exists between two black holes \cite{hawk01}\cite{hows}. In the Lorentz factor, if a particle is travelling faster than the speed of light then $ v^2/c^2 $  would be greater than one. This would give an imaginary number when we take the square root of it. Since the Lorentz factor maps proper time to time this would lead to time becoming imaginary. The suggestion is that this allows particles of matter to exist for arbitrary periods of time outside our region of space time. Indeed, PopÅ‚awski \cite{Poplawski:2010kb} goes further and suggests that all black holes may contain new universes inside that formed simultaneously with another black hole. Since the mathematics of the Lorentz factor are well documented and the idea of a particle moving faster than the speed of light (particles that travel this fast are called tachyons) is not unheard of \cite{Olum:1998mu}, we can trust the concept of imaginary time; however, Hawkingâ€™s interpretation of it needs additional verification as it appears self-published on his website. The fact that Nikodem, PopÅ‚awski and Kip Thorne, as well as Hows with whom I spoke regarding one of Hawkingâ€™s own lectures, all confirm this interpretation shows it to be valid.

Rindlerâ€™s identification also shows that the postulated â€˜baby universesâ€™ are stable solutions to the equations of gravity \cite{Fuller:1962zza} whilst Sopova and Ford \cite{Sopova:2002cs} show that quantum field theory can create negative energy densities which could stabilise wormholes to our own or to other universes \cite{Morris:1988tu}. We can trust their evidence because it was published on topics unrelated to black hole complementarity and so is unlikely to contain bias towards a particular solution. Additionally, their work appears in APSâ€™s Physical Review and Physical Review Letters journal and so will have been independently peer reviewed to ensure its validity.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{Wormhole.png}
    \caption{Our space-time plane is horizontal and imaginary time is vertical. The two black holes link to form a wormhole or Einstein-Rosen black hole. Original image credit: AllenMcC (CC BY-SA 3.0).}
    \label{wormholeDiagram}
\end{figure}

Since baby universes and wormholes in imaginary time are mathematical solutions to equations that have already been formulated that we know to be correct, the baby universe solution is quite favourable when we apply Ockhamâ€™s razor as it doesnâ€™t add a large significant amount to current laws and extends our existing knowledge of physics only slightly. It is therefore relatively simple and the most viable solution examined so far.

\subsection{Contradictory Evidence}
At first, a decrease in information in our universe and its transfer to a â€˜baby universeâ€™ could be seen as a violation of the second law. However this can be resolved by imagining an observer on a larger scale who can perceive both our universe and the child. To them, no information has been lost, only transferred. Preskill gives an argument that as the â€˜baby universeâ€™ is closed no energy enters it. Since the energy and therefore momentum is precisely known then by the uncertainty principle it must have a completely undetermined position that propagates throughout the whole universe \cite{Preskill:1992tc}. We can trust Preskillâ€™s neutrality in this as his report examines multiple solutions to the information paradox without overly focusing and specialising on a particular one. He also states that he finds the idea of `baby universes' ``the most satisfying explanation'', leading us to conclude that if he were biased then it would be favourable towards baby universes, not against. Furthermore, his report is on the proceedings of a conference and so will be heavily influenced by the discussions Preskill had with the other participants and colleagues.

While this may pose a challenge for the theory of baby universes it is again an interpretation based on the uncertainty principle which is a physical law that we already know. Again, this is favourable under Ockhamâ€™s razor since it is much simpler than creating a whole new theory.

\section{Planckian Remnants}
The basis of this theory is that a black hole evaporates away until it reaches a size so small that the basic processes of physics cease to function. Such as scale might involve Planck units â€“ units derived from the fundamental constants of nature rather than prototypes manufactured by humans. Such units, such as the Planck area, tend to be astronomically small and many physicists think this bears a theoretical significance such as being the smallest area of a black hole event horizon.

\subsection{Supporting Evidence}
In their 1992 paper, Banks and Oâ€™Loughlin argue that â€˜hornedâ€™ remnant particles can form through their analysis of string theory \cite{Banks:1992mi}. They suggest that electrically charged black holes have a different metric and therefore different horizon geometry, which can cause â€˜hornsâ€™ in space time. This allows the Hawking-Bekenstein entropy limit to be evaded and so larger amounts of information can be hidden behind said black holes. This is due to the larger horn-like cornucopion geometric extension which offsets the wavelength of the photon used by Bekenstein in his argument \cite{Banks:1992ba}. Having written their argument spread over 2 papers and using a variety of sources, Banks and Oâ€™Loughlin sound convincing. The reliability however is slightly called into question by Banksâ€™ rejection of string theory: ``The String Landscape is a fantasy'' \cite{Banks:2012hx} which may lead him to be biased. Upon cross checking their work I found criticism from Preskill who counter argues that for uncharged black holes, the horns would â€˜pinch offâ€™ and collapse to form a baby universe \cite{Preskill:1992tc}. He also calls into question their use of semiclassical analysis methods. We can trust Preskill as his paper speculates over several of the possible solutions of the black hole information paradox before concluding that none of them are sufficient solutions. His lack of a definitive conclusion means his arguments are likely to be more balanced.

This idea adds to our existing knowledge of black holes by postulating additional geometric entities in their description which depend on the charge of the black hole. This seems quite a complicated theory and so would not be as viable when compared to the theory of baby universe and considered under Ockhamâ€™s razor.

\subsection{Contradictory Evidence}
Physics at these small scales does bring additional problems and many are quick to dismiss the idea of black hole remnants, not because they necessarily disbelieve it but because the theory is not currently well understood. Nobody knows how gravity and quantum mechanics interplay at this scale and if space-time itself takes on a quantised nature. Preskill \cite{Preskill:1992tc} tells us that most theories of Planck mass black holes rely on ``speculation'', going on to say that ``we can not be sure what happens next without a deeper understanding of quantum gravity.''
â€˜t Hooft also voices an objection to the idea of black hole remnants in his analysis of the information that they would have to store. He argues that a black hole has an arbitrary mass when evaporation starts (the choice of when this timeframe is to the observer doesnâ€™t matter) and therefore the remnant must be capable of storing an arbitrarily large amount of information if it is to be conserved \cite{'tHooft:1984re}. This means that Planck-sized remnants with the same masses could potentially store vastly different quantities of information and have infinite states inside them, violating the relationship that Hawking and Bekenstein found between area and information \cite{Preskill:1992tc}. This would seem to contradict a well-established law of physics and so would require significant work to rectify. This cannot be consider viable under Ockham razor as it is not as simple as the theory of complementarity or baby universes, neither of which violate already existing laws of physics.

Both Preskill and â€˜t Hooftâ€™s arguments need to be viewed slightly sceptically as Preskill states from the onset that there are problems with the theory due to currently limited understanding. â€˜t Hooft is an advocate of the holographic principle and so may be â€˜blindedâ€™ to other potential theories. I therefore cross-checked with other authors such as Giddings \cite{Giddings:1995gd} to ensure there is a consensus.

\section{Conclusion}
Having examined the main postulates for solution to the black hole information paradox I can now come to a conclusion about whether black hole complementarity is in fact the most valid and if it is not, which of the others is.

The second law of thermodynamics is deeply ingrained into modern-day physics and to remove, rework or reformulate it would require a complete reconsideration of all of thermal physics and the principles that this theory enshrines. So many other theories are based on the principle of entropy and action that if the second law were changed, considerable work would be needed to ensure these other laws were still correct. Therefore a modification of thermodynamics cannot be considered viable as the amount of complicated new physics that would have to be added would, by the principle of Ockhamâ€™s razor, not be viable.

The idea of the information escaping with the Hawking radiation would require a new mechanism to work out which bits of information have already been radiated away and another mechanism to copy bits of data at the horizon. Both of these ideas represent the addition of new physics and so are less favourable when considered under Ockhamâ€™s razor. However they do offer a solution that does not require the changing of existing physics such as a rewritten second law of thermodynamics; in this case the solution is preferable.

Black hole complementarity has a firm theoretical grounding and uses the principles of string theory and the holographic principle to overcome the information paradox. The claims made by Susskind are well-sourced and his work has been peer reviewed enough to be considered reliable. Whilst complementarity has a solid theoretical foundation, I feel that as a law it would constitute such a paradigm shift in physics that by the principle of Ockhamâ€™s razor, it cannot constitute the most viable solution when compared to the theory of baby universes. The concept that all information in the universe is stored on the boundary of the universe seems so counterintuitive compared to a theory which uses already existing laws of physics.

Theories which involve baby universes tend to be based off an interpretation of physics that we already know and understand; the concept of imaginary time in the Lorentz factor â€“ which is already well understood â€“ and the stability of negative vacuum energies are all derived from other laws of physics. Although Preskill shows that more work is needed to complete the theory, it is, according to Ockhamâ€™s razor, the most viable solution since it doesnâ€™t require any new additions or changes in our current understanding of physics.

The postulate that the information paradox is evaded by storage in Planckian remnants seems to introduce more physical theories dependent on the specific electrical charge of the black hole, leading to a geometry that canâ€™t be described by the Schwarzschild radius. In addition this solution seems to only work for black holes that are electrically charged; a black hole that exists with an overall neutral charge would presumably still experience the paradox. This solution cannot be considered the most viable due to both Ockhamâ€™s razor and its incomplete nature.

As mentioned in my introduction, all scientific theories are rated on their merits when their predictions are compared to experiment. To that end, researchers at Fermi National Accelerator Laboratory in America are carrying out experimentation to find the fundamental unit of space-time associated with the holographic principle \cite{newsci}. It would seem incorrect to dismiss black hole complementarity as a theory until the results of this experiment have been published and reviewed by the community; in this extended project all I can conclude is that black hole complementarity does not offer the most viable solution according to Ockhamâ€™s razor when compared to the other solutions.

In answer to the title question, I conclude that the theory of formation of baby universes, not black hole complementarity, is the most viable solution to the black hole information paradox.

\bibliographystyle{plain}
\bibliography{references}

\noindent
Figure~\ref{wormholeDiagram} on page~\pageref{wormholeDiagram} is an adaptation by myself of the file \url{http://upload.wikimedia.org/wikipedia/commons/d/d7/LorentzianWormhole.jpg} from the Wikimedia Commons online media repository and is licensed under the Creative Commons Attribution-Share Alike 3.0 Unported license by its author, AllenMcC. A complete copy of the license text can be found at \url{http://creativecommons.org/licenses/by-sa/3.0/legalcode}.
\\\\
\noindent
This document typeset in \LaTeX\! by C. H. T. Jonas. For errata see \url{http://www.charliejonas.co.uk}.

\end{document}